from inspect import cleandoc
import torch
import torch.nn.functional as F
from typing import List, Tuple
import os
from pathlib import Path
import json
from PIL import Image

try:
    import kornia
    import kornia.enhance as KE
    _HAS_KORNIA = True
except Exception:
    _HAS_KORNIA = False
    
try:
    import numpy as np
    from skimage.exposure import match_histograms
    _HAS_SKIMAGE = True
except Exception:
    _HAS_SKIMAGE = False
    
try:
    import cv2
    _HAS_CV2 = True
except Exception:
    _HAS_CV2 = False


def load_image_comfyui(image_path):
    """Load image from disk into ComfyUI IMAGE format [1,H,W,C] float32 0..1"""
    try:
        img = Image.open(image_path).convert("RGBA")
        img_tensor = torch.from_numpy(np.array(img)).to(dtype=torch.float32) / 255.0  # [H,W,C] float32 0..1
        img_tensor = img_tensor.unsqueeze(0)  # [1,H,W,C]
    except (FileNotFoundError, OSError, AttributeError) as e:
        img_tensor = make_empty_image()
        
    return img_tensor

def _nhwc_to_nchw(x: torch.Tensor) -> torch.Tensor:
    return x.permute(0, 3, 1, 2).contiguous()

def _nchw_to_nhwc(x: torch.Tensor) -> torch.Tensor:
    return x.permute(0, 2, 3, 1).contiguous()

def kornia_unsharp(img_nhwc, radius=1.5, amount=0.5):
    # img_nhwc: [B,H,W,C] in 0..1, C=3 or 4
    has_a = (img_nhwc.shape[-1] == 4)
    rgb = img_nhwc[...,:3]
    x = _nhwc_to_nchw(rgb)  # -> [B,3,H,W]
    # gaussian blur (sigma ~ radius), then unsharp
    sigma = max(0.1, float(radius))
    k = KE.gaussian_blur2d(x, kernel_size=int(2*round(3*sigma)+1), sigma=(sigma, sigma))
    sharp = torch.clamp(x + amount*(x - k), 0, 1)
    out = _nchw_to_nhwc(sharp)
    if has_a:
        out = torch.cat([out, img_nhwc[...,3:4]], dim=3)
    return out

def kornia_clahe(img_nhwc, clip_limit=2.0, grid=(8,8)):
    x = _nhwc_to_nchw(img_nhwc[...,:3])
    y = KE.equalize_clahe(x, clip_limit=clip_limit, grid_size=grid)
    out = _nchw_to_nhwc(y)
    return torch.cat([out, img_nhwc[...,3:4]], dim=3) if img_nhwc.shape[-1]==4 else out

def skimage_match_hist(img_nhwc, ref_nhwc, multichannel=True, amount=1.0):
    # Move to CPU/HWC uint8
    def to_u8(hwc):
        if hwc.dtype.is_floating_point:
            hwc = (hwc.clamp(0,1)*255.0).to(torch.uint8)
        return hwc

    i = to_u8(img_nhwc[0,...,:3].detach().cpu())   # [H,W,3]
    r = to_u8(ref_nhwc[0,...,:3].detach().cpu())

    matched = match_histograms(i.numpy(), r.numpy(), channel_axis=-1 if multichannel else None)
    matched = torch.from_numpy(matched).to(dtype=torch.float32)/255.0
    matched = matched.unsqueeze(0)  # [1,H,W,3]

    if img_nhwc.shape[-1] == 4:
        matched = torch.cat([matched, img_nhwc[... ,3:4].detach().cpu()], dim=3)

    # blend with original if amount<1
    out = img_nhwc.detach().cpu()
    out[...,:3] = out[...,:3].lerp(matched[...,:3], float(amount))
    return out.to(img_nhwc.device, dtype=img_nhwc.dtype)

def _stack_if_same_shape(frames: List[torch.Tensor]) -> torch.Tensor:
    if len(frames) == 0 or frames is None:
        raise ValueError("No frames to stack")
    h0, w0, c0 = frames[0].shape[1], frames[0].shape[2], frames[0].shape[3]

    for f in frames:
        hCur, wCur, cCur = f.shape[1], f.shape[2], f.shape[3]
        if f.ndim != 4 or hCur != h0 or wCur != w0 or cCur != c0:
            raise ValueError("All frames must have the same shape to stack")
    return torch.cat(frames, dim=0) # should this use torch.stack?  # [B, H, W, C]

def _compute_ref_stats(frames: List[torch.Tensor], window: int) -> Tuple[torch.Tensor, torch.Tensor, float]:
    """
    Compute per-channel mean/std (RGB) and luma mean over the last `window` frames of the list.
    Each frame is [1,H,W,C] NHWC in [0,1].

    Args:
        frames (List[torch.Tensor]): _description_
        window (int): _description_

    Raises:
        ValueError: _description_
        ValueError: _description_
        ValueError: _description_
        ValueError: _description_

    Returns:
        mean_c: [1,1,1,3] mean per channel RGB
        std_c: [1,1,1,3] std per channel RGB (clamped to >= 1e-6)
        mean_luma: float mean luma Y in [0,1]
    """
    if not frames:
        # Fallback to neutral stats
        mean_c = torch.tensor([[[[0.5, 0.5, 0.5]]]], dtype=torch.float32)
        std_c = torch.tensor([[[[0.25, 0.25, 0.25]]]], dtype=torch.float32)
        mean_luma = 0.5
        return mean_c, std_c, mean_luma
    
    sub = frames[-window:] if window > 0 else frames
    batch = torch.cat(sub, dim=0)  # [B, H, W, C]
    mean_c = batch.mean(dim=[0,1,2], keepdim=True)  # [1,1,1,C]
    std_c = batch.std(dim=[0,1,2], keepdim=True).clamp_min(1e-6)  # [1,1,1,C]
    
    r, g, b = mean_c[..., 0], mean_c[..., 1], mean_c[..., 2]
    mean_luma = (0.2126 * r + 0.7152 * g + 0.0722 * b).item()  # scalar in [0,1]
    return mean_c, std_c, mean_luma

def _pick_ref_image(frames: List[torch.Tensor], window: int) -> torch.Tensor:
    """Pick one frame (the median index of the last `window`) as histogram reference."""
    if not frames:
        return None

    sub = frames[-window:] if window > 0 else frames
    idx = len(sub) // 2
    return sub[idx]  # [1,H,W,C]


# ------------------ Processing primitives (NHWC) ------------------

def proc_deflicker_luma(img: torch.Tensor, target_luma: float, strength: float) -> torch.Tensor:
    # scale RGB together to match target mean luma
    r, g, b = img[...,:3].unbind(dim=3)
    luma = 0.2126*r + 0.7152*g + 0.0722*b
    mean = luma.mean().item()
    if mean <= 1e-6:
        return img
    s = 1.0 + strength * (target_luma/mean - 1.0)
    out_rgb = (img[...,:3] * s).clamp(0,1)
    return torch.cat([out_rgb, img[...,3:4]], dim=3) if img.shape[3] == 4 else out_rgb

def proc_deflicker_clahe(img: torch.Tensor, clip_limit: float, grid_w: int, grid_h: int) -> torch.Tensor:
    if not _HAS_KORNIA:
        return img  # fallback silently if kornia missing
    # CLAHE works best per-channel in NCHW
    x = img[...,:3].permute(0,3,1,2).contiguous()  # [B,3,H,W]
    y = KE.equalize_clahe(x, clip_limit=float(clip_limit), grid_size=(int(grid_h), int(grid_w)))
    out = y.permute(0,2,3,1).contiguous()
    return torch.cat([out, img[...,3:4]], dim=3) if img.shape[3]==4 else out

def proc_color_meanstd(img: torch.Tensor, tgt_mean: torch.Tensor, tgt_std: torch.Tensor, amount: float) -> torch.Tensor:
    x = img[...,:3]
    im_mean = x.mean(dim=(0,1,2), keepdim=True)
    im_std  = x.std(dim=(0,1,2), keepdim=True).clamp_min(1e-6)
    matched = ((x - im_mean)/im_std) * tgt_std.to(x) + tgt_mean.to(x)
    out = x.lerp(matched, float(amount)).clamp(0,1)
    return torch.cat([out, img[...,3:4]], dim=3) if img.shape[3]==4 else out

def proc_color_histmatch(img: torch.Tensor, ref: torch.Tensor, amount: float) -> torch.Tensor:
    if not _HAS_SKIMAGE:
        return img
    # to CPU uint8 HWC
    i = img[0,...,:3].detach().clamp(0,1).mul(255).to(torch.uint8).cpu().numpy()  # HWC
    r = ref[0,...,:3].detach().clamp(0,1).mul(255).to(torch.uint8).cpu().numpy()
    matched = match_histograms(i, r, channel_axis=-1)
    matched = torch.from_numpy(matched).to(dtype=torch.float32)/255.0
    matched = matched.unsqueeze(0)  # [1,H,W,3]
    out_rgb = img[...,:3].lerp(matched.to(img.device, img.dtype), float(amount)).clamp(0,1)
    return torch.cat([out_rgb, img[...,3:4]], dim=3) if img.shape[3]==4 else out_rgb

def proc_unsharp(img: torch.Tensor, radius: float, amount: float) -> torch.Tensor:
    if not _HAS_KORNIA:
        return img
    x = img[...,:3].permute(0,3,1,2).contiguous()  # NCHW
    sigma = max(0.1, float(radius))
    ksize = int(2*round(3*sigma)+1)
    blur = KE.gaussian_blur2d(x, kernel_size=ksize, sigma=(sigma, sigma))
    sharp = (x + amount*(x - blur)).clamp(0,1)
    out = sharp.permute(0,2,3,1).contiguous()
    return torch.cat([out, img[...,3:4]], dim=3) if img.shape[3]==4 else out

def proc_bilateral_cv2(img: torch.Tensor, d: int, sigma_color: float, sigma_space: float) -> torch.Tensor:
    if not _HAS_CV2:
        return img
    # CPU path for cv2
    hwc = img[0].detach().clamp(0,1).cpu().numpy()
    rgb = (hwc[...,:3]*255.0).astype('uint8')
    out = cv2.bilateralFilter(rgb, d=int(d), sigmaColor=float(sigma_color), sigmaSpace=float(sigma_space))
    out = torch.from_numpy(out).to(dtype=img.dtype)/255.0
    out = out.unsqueeze(0)  # [1,H,W,3]
    return torch.cat([out, img[...,3:4].detach().cpu()], dim=3).to(img.device, img.dtype) if img.shape[3]==4 else out.to(img.device, img.dtype)

class SAMPreprocessNHWC:
    """
    Prepare IMAGE for SAM predictor inside other nodes:
      - Ensure RGB (drop alpha)
      - Resize so long side == 1024 (keeps aspect)
      - Scale to 0..1 float32
      - Return NHWC back (ComfyUI IMAGE), which the next node will convert as needed
    """
    @classmethod
    def INPUT_TYPES(cls):
        return {"required": {"image": ("IMAGE",)}}

    RETURN_TYPES = ("IMAGE", "STRING")
    RETURN_NAMES = ("image", "info")
    FUNCTION = "samtastic"
    CATEGORY = "ðŸ§Š frost-byte/Preprocessing"

    def samtastic(self, image):
        if image.ndim != 4:
            raise RuntimeError("IMAGE must be [B,H,W,C]")

        print(f"SAMPreprocessNHWC: image in shape={image.shape}")        

        b, h, w, c = image.shape
        img = image

        # drop alpha if present
        if c == 4:
            img = img[..., :3]
            c = 3
        if c != 3:
            raise RuntimeError(f"SAM expects RGB 3ch, got {c}")

        # convert to float32, scale to 0..255 (SAM torch path often expects that)
        img = img.to(torch.float32).clamp(0, 1)

        # resize so max(H,W)=1024 with aspect
        long_side = max(h, w)
        if long_side != 1024:
            scale = 1024.0 / long_side
            new_h, new_w = int(round(h * scale)), int(round(w * scale))
            img = F.interpolate(
                img.permute(0, 3, 1, 2),  # NHWC -> NCHW for interpolate
                size=(new_h, new_w),
                mode="bilinear",
                align_corners=False
            ).permute(0, 2, 3, 1).contiguous()  # back to NHWC
            #.contiguous()  # we do not want to go back to NHWC, output needs to be NCHW for SAM predictor
# AssertionError: set_torch_image input must be BCHW with long side 1024
# /home/beerye/comfyui_env/.venv/lib/python3.12/site-packages/segment_anything/predictor.py", line 80, in set_torch_image
        info = f"[SAMPreprocessNHWC] out={tuple(img.shape)} range=[{img.min().item():.1f},{img.max().item():.1f}]"
        print(info)
        return (img, info)

class TailEnhancePro:
    """
    TailEnhancePro:
      - Split last K frames of a LIST[IMAGE], run selected processing chain on them, recombine.
      - Processing toggles + parameters:
          * Deflicker: luma-scale OR CLAHE
          * Color match: histogram OR mean/std affine (with blend amount)
          * Sharpen: unsharp mask (kornia)
          * Denoise: bilateral (opencv)
      - Reference window: how many HEAD frames to compute stats / pick histogram reference from.
    """
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "frames": ("LIST",),
                "tail_count": ("INT", {"default": 6, "min": 1, "max": 999}),
                "ref_window": ("INT", {"default": 24, "min": 1, "max": 999}),
                "ref_from_head": ("BOOLEAN", {"default": True}),  # if False, uses tail as fallback source
                # Deflicker
                "enable_deflicker": ("BOOLEAN", {"default": True}),
                "deflicker_mode": (["luma_scale", "clahe"],),
                "deflicker_strength": ("FLOAT", {"default": 0.5, "min": 0.0, "max": 1.0, "step": 0.05}),
                "clahe_clip_limit": ("FLOAT", {"default": 2.0, "min": 0.1, "max": 10.0, "step": 0.1}),
                "clahe_grid_w": ("INT", {"default": 8, "min": 2, "max": 64}),
                "clahe_grid_h": ("INT", {"default": 8, "min": 2, "max": 64}),
                # Color
                "enable_color_match": ("BOOLEAN", {"default": True}),
                "color_mode": (["histogram", "meanstd"],),
                "color_amount": ("FLOAT", {"default": 0.6, "min": 0.0, "max": 1.0, "step": 0.05}),
                # Sharpen
                "enable_unsharp": ("BOOLEAN", {"default": True}),
                "unsharp_radius": ("FLOAT", {"default": 1.5, "min": 0.1, "max": 10.0, "step": 0.1}),
                "unsharp_amount": ("FLOAT", {"default": 0.5, "min": 0.0, "max": 3.0, "step": 0.05}),
                # Denoise
                "enable_bilateral": ("BOOLEAN", {"default": False}),
                "bilateral_d": ("INT", {"default": 5, "min": 1, "max": 25}),
                "bilateral_sigma_color": ("FLOAT", {"default": 25.0, "min": 1.0, "max": 250.0, "step": 1.0}),
                "bilateral_sigma_space": ("FLOAT", {"default": 7.0, "min": 1.0, "max": 100.0, "step": 1.0}),
            }
        }

    RETURN_TYPES = ("LIST", "IMAGE", "STRING")
    RETURN_NAMES = ("frames", "batched", "info")
    FUNCTION = "tail_enhance_pro"
    CATEGORY = "ðŸ§Š frost-byte/Video"

    def tail_enhance_pro(self, frames, tail_count, ref_window, ref_from_head,
        enable_deflicker, deflicker_mode, deflicker_strength, clahe_clip_limit, clahe_grid_w, clahe_grid_h,
        enable_color_match, color_mode, color_amount,
        enable_unsharp, unsharp_radius, unsharp_amount,
        enable_bilateral, bilateral_d, bilateral_sigma_color, bilateral_sigma_space):

        info_msgs = []
        if not frames or len(frames) == 0:
            return ([], None, "[TailEnhancePro] empty input")

        n = len(frames)
        k = max(1, min(int(tail_count), n))
        head = frames[: n - k]
        tail = frames[n - k :]

        # Reference set
        ref_src = head if (ref_from_head and len(head) > 0) else (tail if len(tail) > 0 else frames)
        mean_c, std_c, mean_luma = _compute_ref_stats(ref_src, ref_window)
        ref_img_for_hist = _pick_ref_image(ref_src, ref_window)

        if enable_deflicker and deflicker_mode == "clahe" and not _HAS_KORNIA:
            info_msgs.append("CLAHE requested but kornia not installed -> skipped")
        if enable_color_match and color_mode == "histogram" and not _HAS_SKIMAGE:
            info_msgs.append("Histogram match requested but scikit-image not installed -> skipped")
        if enable_bilateral and not _HAS_CV2:
            info_msgs.append("Bilateral requested but opencv-python not installed -> skipped")

        out_tail = []
        for img in tail:
            x = img
            if enable_deflicker:
                if deflicker_mode == "luma_scale":
                    x = proc_deflicker_luma(x, mean_luma, deflicker_strength)
                else:
                    x = proc_deflicker_clahe(x, clahe_clip_limit, clahe_grid_w, clahe_grid_h)

            if enable_color_match:
                if color_mode == "histogram" and ref_img_for_hist is not None and _HAS_SKIMAGE:
                    x = proc_color_histmatch(x, ref_img_for_hist, color_amount)
                else:
                    x = proc_color_meanstd(x, mean_c.to(x), std_c.to(x), color_amount)

            if enable_bilateral:
                x = proc_bilateral_cv2(x, bilateral_d, bilateral_sigma_color, bilateral_sigma_space)

            if enable_unsharp:
                x = proc_unsharp(x, unsharp_radius, unsharp_amount)

            out_tail.append(x.clamp(0,1))

        out_frames = list(head) + out_tail
        batched = _stack_if_same_shape(out_frames)
        msg = f"[TailEnhancePro] n={n} tail={k} ref_window={ref_window} " \
              f"ops(deflicker={enable_deflicker}:{deflicker_mode}, color={enable_color_match}:{color_mode}, " \
              f"bilateral={enable_bilateral}, unsharp={enable_unsharp})"
        if info_msgs:
            msg += " | " + " ; ".join(info_msgs)
        return (out_frames, batched, msg)

class TailSplit:
    """
    Splits the input image batch into two parts: the main part and a tail part.
    The tail part is defined as the last `tail_size` images in the batch.
    - IMAGE is expected as [B, H, W, C],
    - Returns:
        - main_image: [B - tail_size, H, W, C]
        - tail_image: [tail_size, H, W, C]
    """
    
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "image": ("IMAGE", {"tooltip": "Input image batch"}),
                "tail_size": ("INT", {
                    "default": 5,
                    "min": 5,
                    "max": 100,
                    "step": 1,
                    "display": "number",
                    "tooltip": "Number of images to include in the tail part"
                }),
                "debug": ("BOOLEAN", {
                    "default": False,
                    "tooltip": "If true, will print debug info to console"
                }),
            },
        }
    RETURN_TYPES = ("IMAGE", "IMAGE", "STRING")
    RETURN_NAMES = ("main_image", "tail_image", "debug_info")
    FUNCTION = "split_tail"
    CATEGORY = "ðŸ§Š frost-byte/Video"
    
    def split_tail(self, image, tail_size=1, debug=False):
        # image: torch.FloatTensor [B, H, W, C]
        if not torch.is_tensor(image):
            raise ValueError("Input 'image' must be a torch tensor")

        print(f"TailSplit: image in shape={image.shape}, tail_size={tail_size}, dtype={image.dtype}, device={image.device}") if debug else None        
        b, h, w, c = image.shape
        print(f"TailSplit: b={b}, h={h}, w={w}, c={c}") if debug else None
        
        if tail_size >= b:
            raise ValueError("tail_size must be less than the batch size")
        
        main_image = image[:-tail_size]  # [B - tail_size, H, W, C]
        tail_image = image[-tail_size:]   # [tail_size, H, W, C]
        
        try:
            mn = image.detach().min().item()
            mx = image.detach().max().item()
            alpha_summary = f" range=[{mn:.6f},{mx:.6f}]"
        except Exception:
            alpha_summary = ""
            
        msg = (
            f"TailSplit: image in shape={image.shape}, tail_size={tail_size}, dtype={image.dtype}, device={image.device}, "
            f"-> main_image shape={main_image.shape}, tail_image shape={tail_image.shape}{alpha_summary}"
        )
        
        if debug:
            print(msg)

        return (main_image, tail_image, msg)

class OpaqueAlpha:
    """
    Creates an opaque mask (all 1.0) matching the input image's spatial size and applies it
    as an alpha channel to the input image. Handles RGB or RGBA input images and batches.
    - IMAGE is expected as [B, C, H, W], float 0..1
    - Returns:
        - image_rgba: [B, 4, H, W]
        - mask: [B, 1, H, W] (float 0..1)
    """
    
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "image": ("IMAGE", {"tooltip": "Input image, RGB or RGBA"}),
                "alpha_value": ("FLOAT", {
                    "default": 1.0,
                    "min": 0.0,
                    "max": 1.0,
                    "step": 0.01,
                    "round": 0.001,
                    "display": "number",
                    "tooltip": "Alpha value to set in the mask"
                }),
                "force_replace_alpha": ("BOOLEAN", {
                    "default": True,
                    "tooltip": "If true, will replace existing alpha channel if input image is RGBA"
                }),
                "debug": ("BOOLEAN", {
                    "default": False,
                    "tooltip": "If true, will print debug info to console"
                }),
            },
        }
    RETURN_TYPES = ("IMAGE", "MASK", "STRING")
    RETURN_NAMES = ("image_rgba", "mask", "debug_info")
    FUNCTION = "apply_opaque_alpha"
    CATEGORY = "ðŸ§Š frost-byte/Image Processing"

    def apply_opaque_alpha(self, image, alpha_value=1.0, force_replace_alpha=True, debug=False):
        # image: torch.FloatTensor [B, H, W, C], C=3 or 4, float 0..1
        if not torch.is_tensor(image):
            raise ValueError("Input 'image' must be a torch tensor")

        print(f"OpaqueAlpha: image in shape={image.shape}, alpha_value={alpha_value}, force_replace_alpha={force_replace_alpha},dtype={image.dtype}, device={image.device}") if debug else None        
        b, h, w, c = image.shape
        print(f"OpaqueAlpha: b={b}, h={h}, w={w}, c={c}") if debug else None
        device = image.device
        dtype = image.dtype
        
        # Build an opaque mask [B, H, W, 1]
        mask = torch.full((b, h, w, 1), fill_value=alpha_value, device=device, dtype=dtype)
        
        if c == 4:
            if force_replace_alpha:
                # Replace existing alpha channel
                image_rgba = image.clone
                image_rgba[:, :, :, 3:4] = mask
            else:
                # Keep existing alpha channel
                image_rgba = image
        elif c == 3:
            # Add alpha channel
            image_rgba = torch.cat([image, mask], dim=3)  # [B, H, W, 4]
        else:
            raise ValueError("Input 'image' must have 3 (RGB) or 4 (RGBA) channels")
        
        try:
            mn = image.detach().min().item()
            mx = image.detach().max().item()
            alpha_summary = f" alpha_range=[{mn:.6f},{mx:.6f}]"
        except Exception:
            alpha_summary = ""
            
        msg = (
            f"OpaqueAlpha: image in shape={image.shape}, alpha_value={alpha_value}, force_replace_alpha={force_replace_alpha},dtype={image.dtype}, device={image.device}, "
            f"range=[{mn:.6f},{mx:.6f}] -> image out shape={image_rgba.shape}, mask shape={mask.shape}{alpha_summary}"
        )
        
        if debug:
            print(msg)

        return (image_rgba,mask,msg)

class SubdirLister:
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "directory_path": ("STRING", {"default": "", "tooltip": "Path to the directory", "multiline": False}),
            }
        }
    RETURN_TYPES = ("DICT","STRING")
    RETURN_NAMES = ("dir_dict","dir_names")
    OUTPUT_IS_LIST = (False, True)
    FUNCTION = "list_subdirectories"
    CATEGORY = "ðŸ§Š frost-byte/File"
    
    def list_subdirectories(self, directory_path):

        subdir_dict = {}

        if not os.path.isdir(directory_path):
            return (subdir_dict, [])

        with os.scandir(directory_path) as entries:
            for entry in entries:
                if entry.is_dir():
                    subdir_dict[entry.name] = entry.path

        return (subdir_dict, list(subdir_dict.keys()))

def load_prompt_json(prompt_json_path):
    if not os.path.isfile(prompt_json_path):
        print(f"ScenePipe: prompt_json_path '{prompt_json_path}' is not a valid file")
        return {"girl_pos": "", "male_pos": ""}

    output = {}
    try:
        with open(prompt_json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        output["girl_pos"] = data.get("girl_pos", "")
        output["male_pos"] = data.get("male_pos", "")
        return output
    except Exception as e:
        print(f"ScenePipe: Error loading prompt JSON from '{prompt_json_path}': {e}")
        return {"girl_pos": "", "male_pos": ""}

def load_json_file(json_path):
    if not os.path.isfile(json_path):
        print(f"ScenePipe: json_path '{json_path}' is not a valid file")
        return ""

    try:
        with open(json_path, 'r', encoding='utf-8') as f:
            data = f.read()
        return data
    except Exception as e:
        print(f"ScenePipe: Error loading JSON from '{json_path}': {e}")
        return ""

def make_empty_image(batch=1, height=64, width=64, channels=3):
    """
    Returns a blank image tensor in ComfyUI format:
    - Shape: [B, H, W, C]
    - Dtype: torch.float32
    - Values: 0.0 (black) in range [0.0, 1.0]
    """
    return torch.zeros((batch, height, width, channels), dtype=torch.float32)

class ScenePipe:
    @classmethod
    def INPUT_TYPES(s):
        return {
            "optional": {
                "scene_pipe": ("SCENE_PIPE",),
                "pose_name": ("STRING", {"default": "", "tooltip": "Name of the pose to load (without suffixes)", "multiline": False}),
                "pose_dir": ("STRING", {"default": "", "tooltip": "Directory containing the files for the pose", "multiline": False}),
            }
        }
        
    RETURN_TYPES = (
        "SCENE_PIPE",
        "IMAGE",
        "IMAGE",
        "IMAGE",
        "IMAGE",
        "IMAGE",
        "IMAGE",
        "IMAGE",
        "STRING",
        "STRING",
        "STRING",
        "DICT",
    )
    RETURN_NAMES = (
        "scene_pipe",
        "depth_image",
        "depth_midas_image",
        "depth_any_image",
        "dense_pose_image",
        "pose_image",
        "canny_image",
        "base_image",
        "pose_json",
        "pose_name",
        "pose_dir",
        "scene_dict",
    )
    FUNCTION = "process_scene_pipe"
    CATEGORY = "ðŸ§Š frost-byte/Scene"

    def process_scene_pipe(
        self,
        scene_pipe=None,
        pose_name="",
        pose_dir="",
    ):
        if pose_dir and not os.path.isdir(pose_dir):
            pose_dir = ""
            print(f"ScenePipe: pose_dir '{pose_dir}' is not a valid directory, resetting to empty")
        
        if pose_name == "":
            print("ScenePipe: pose_name is empty, cannot load pose files")

        depth_image = None
        depth_midas_image = None
        depth_any_image = None
        dense_pose_image = None
        pose_image = None
        canny_image = None
        base_image = None
        pose_json = ""
        scene_dict = {"girl_pos": "", "male_pos": ""}

        if os.path.isdir(pose_dir):
            pose_path = Path(pose_dir)
            depth_path = pose_path / (pose_name + "_depth.png")
            pose_json_path = pose_path / (pose_name + "_pose.json")
            pose_image_path = pose_path / (pose_name + "_pose.png")
            prompts_path = pose_path / (pose_name + "_prompts.json")
            dense_path = pose_path / "pose_dense_0001.png"
            midas_path = pose_path / "depth_midas_0001.png"
            any_path = pose_path / "depth_any_0001.png"
            canny_path = pose_path / "canny_0001.png"
            img_path = pose_path / "upscale_0001.png"
            

            if depth_path.is_file():
                depth_image = load_image_comfyui(depth_path)
            if midas_path.is_file():
                depth_midas_image = load_image_comfyui(midas_path)
            if any_path.is_file():
                depth_any_image = load_image_comfyui(any_path)
            if dense_path.is_file():
                dense_pose_image = load_image_comfyui(dense_path)
            if pose_image_path.is_file():
                pose_image = load_image_comfyui(pose_image_path)
            if canny_path.is_file():
                canny_image = load_image_comfyui(canny_path)
            if img_path.is_file():
                base_image = load_image_comfyui(img_path)
            if pose_json_path.is_file():
                pose_json = load_json_file(pose_json_path)
            if prompts_path.is_file():
                scene_dict = load_prompt_json(prompts_path)
                print(f"ScenePipe: Loaded scene_dict from '{prompts_path}' -> {scene_dict}")

        pipe = {
            "scene_pipe": scene_pipe,
            "depth_image": depth_image,
            "depth_midas_image": depth_midas_image,
            "depth_any_image": depth_any_image,
            "dense_pose_image": dense_pose_image,
            "pose_image": pose_image,
            "canny_image": canny_image,
            "base_image": base_image,
            "pose_json": pose_json,
            "pose_name": pose_name,
            "pose_dir": pose_dir,
            "scene_dict": scene_dict,
        }
        
        return (
            pipe,
            depth_image,
            depth_midas_image,
            depth_any_image,
            dense_pose_image,
            pose_image,
            canny_image,
            base_image,
            pose_json,
            pose_name,
            pose_dir,
            scene_dict,
        )
    
# A dictionary that contains all nodes you want to export with their names
# NOTE: names should be globally unique
NODE_CLASS_MAPPINGS = {
    "OpaqueAlpha": OpaqueAlpha,
    "TailEnhancePro": TailEnhancePro,
    "TailSplit": TailSplit,
    "SAMPreprocessNHWC": SAMPreprocessNHWC,
    "SubdirLister": SubdirLister,
    "ScenePipe": ScenePipe,
}
# A dictionary that contains the friendly/humanly readable titles for the nodes
NODE_DISPLAY_NAME_MAPPINGS = {
    "OpaqueAlpha": "Opaque Alpha (make mask + apply)",
    "TailEnhancePro": "Tail Enhance Pro",
    "TailSplit": "Tail Split",
    "SAMPreprocessNHWC": "SAM Preprocess (NHWC RGB, long=1024, 0..255)",
    "SubdirLister": "Subdirectory Lister",
    "ScenePipe": "Scene Pipe",
}
